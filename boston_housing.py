# -*- coding: utf-8 -*-
"""Boston_Housing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P_MBL3VPlFIyBdgFCNVoyv2sA3EBYMLg
"""

#importing desired dependencies
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#loading our Boston Housing Dataset
df = pd.read_csv("boston.csv")
print(df.head(5))

## Data Preprocessing

#Handling Missing values
print("No. of null values in columns\n")
print(df.isna().sum())

#Changing names of the columns for better understanding
col_names = {
    'CRIM': 'Crime_Rate',
    'ZN': 'Land_Zoning',
    'INDUS': 'Non_Retail',
    'CHAS': 'River',
    'NOX': 'NO_Concentration',
    'RM': 'Avg_Rooms',
    'AGE': 'Building_Age',
    'DIS': 'Dist_To_Employment',
    'RAD': 'Radial_Access',
    'TAX': 'Tax_Rate',
    'PTRATIO': 'Pupil_Teacher_Ratio',
    'B': 'Black_Proportion',
    'LSTAT': 'Lower_Status',
    'MEDV': 'Med_House_Value'
}

# Renaming the columns
df = df.rename(columns=col_names)

# Summary of the dataset
print("Summary of the whole Dataset:\n")
print(df.describe())

# Checking Data types to change it if neccessary from categorical to numerical

print("Data types:\n")
print(df.info())

# Exploratory Data Analysis

# Explorinng our data for more analysis


# Calculating Correlation

corr_matrix = df.corr()

# Visualising the correlation using heatmap

plt.figure(figsize=(14,8))
plt.title("Correlation Heatmap")
sns.heatmap(corr_matrix, annot=True, cmap="RdBu",  fmt=".2f")
plt.show()

# Deciding Target Variable/ Output variable
target = "Med_House_Value"
y = df[target]

# Input variable except the output feature in our case "Med_House_Value "
X = df.drop(target, axis=1)

# Data Splitting into training and testing

from sklearn.model_selection import train_test_split

# Splitting data into 30-70 % for testing-training respectively
# random state used for shuffling which must be kept constant otherwise disrupts the model

x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=46)

#model training

from sklearn.linear_model import LinearRegression

reg = LinearRegression()

reg.fit(x_train, y_train)

# Model testing/predicting

y_pred = reg.predict(x_test)

# Measuring Performance Metrics
# y_pred -----> data predicted from testing data of X values/ prediction on actual X values
# y_test ---> data of Med_Values given already / actual values


from sklearn import metrics
from sklearn.metrics import r2_score

# absolute avg difference between actual and predicted values
MAE = metrics.mean_absolute_error(y_test, y_pred)

# square of avg difference between actual and predicted values
MSE = metrics.mean_squared_error(y_test, y_pred)

# sq root of MSE
RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))

# Lower values means our model is much accurate
print("\n\n\n")
print("Mean Absolute Error: ", MAE)
print("Mean Squared Error: ", MSE)

print("Root Mean Squared Error: ", RMSE)

print("Training score: ",round(reg.score(x_train, y_train)*100,2))
print("Testing score: ",round(reg.score(x_test, y_test)*100,2))

print("\n\n\n")

# Displaying the difference

df2 = pd.DataFrame({"Actual values":y_test, "Predicted Values": y_pred, "Difference": y_test-y_pred})
print(df2.head(5))
print()

# Checking whether our model predicting right
# ---> Actual value was 36
# ----> Our model predicted 27.35, a difference of 9 values

# Predicting price given in dataset
result = reg.predict([[0.06905,0.0,2.18,0,0.458,7.147,54.2,6.0622,3,222.0,18.7,396.90,5.33]])
print("Result: ", result[0])


# importing model into local machine so we could use with GUI applications

# import pickle
#
# filename = 'House_Price_Prediction.pkl'
# pickle.dump(reg, open(filename, 'wb'))